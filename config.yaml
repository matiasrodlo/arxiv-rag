# RAG System Configuration

# Paths
paths:
  pdf_dir: "papers"
  metadata_dir: "papers"
  output_dir: "output"
  vector_db_path: "data/vector_db"
  extracted_text_dir: "output"
  progress_db: "data/progress.db"  # SQLite database for progress tracking

# PDF Extraction
pdf_extraction:
  primary_library: "pymupdf"  # pymupdf, pdfplumber, pypdf
  enable_ocr: true
  ocr_language: "eng"
  min_text_length: 100  # Minimum characters to consider valid
  extract_metadata: true
  extract_references: true

# Text Processing
text_processing:
  remove_headers_footers: true
  normalize_whitespace: true
  fix_encoding: true
  improve_formulas: true  # Improve mathematical formula formatting
  min_chunk_size: 200  # Minimum characters per chunk (increased for better quality)
  max_chunk_size: 2000  # Maximum characters per chunk (increased for better context, was 1000)
  chunk_overlap: 400  # Overlap between chunks in characters (increased, was 200)
  preserve_sections: true

# Chunking
chunking:
  method: "semantic"  # semantic, fixed, sentence
  chunk_size: 1024  # tokens (increased for better context, was 512)
  chunk_overlap: 100  # tokens (increased for better continuity, was 50)
  model: "sentence-transformers/all-MiniLM-L6-v2"  # For semantic chunking

# Embeddings
embeddings:
  model: "sentence-transformers/all-mpnet-base-v2"  # High-quality model for 128GB RAM
  batch_size: 128  # Optimized for M4 Max with 128GB RAM (was 32)
  device: "mps"  # Use Metal Performance Shaders (Apple GPU) for maximum speed
  normalize_embeddings: true
  generate_during_processing: false  # Set to false for large-scale: generate embeddings in separate batch step

# Vector Database
vector_db:
  type: "chroma"  # chroma, qdrant
  collection_name: "arxiv_cs_papers"
  persist_directory: "data/vector_db"
  add_during_processing: false  # Set to false for large-scale: add to vector store in separate batch step
  # For Qdrant:
  # qdrant_host: "localhost"
  # qdrant_port: 6333

# Retrieval
retrieval:
  top_k: 10
  use_hybrid_search: true
  hybrid_alpha: 0.7  # Weight for semantic search (0.7) vs keyword (0.3)
  use_reranking: true
  rerank_top_k: 50  # Re-rank top 50, return top 10
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# Processing
processing:
  batch_size: 1000  # Papers per batch (optimized for 800K papers, was 500)
  num_workers: 12  # Parallel workers (optimized for long-run stability: 14 workers showed 98.2% CPU peaks, reduced to 12 for better stability over 9+ day runs)
  max_retries: 3
  retry_delay: 1  # seconds
  skip_processed: true  # Skip papers that are already processed
  checkpoint_interval: 100  # Save progress every N papers

# Logging
logging:
  level: "INFO"
  log_file: "logs/rag_pipeline.log"
  log_rotation: "100 MB"

